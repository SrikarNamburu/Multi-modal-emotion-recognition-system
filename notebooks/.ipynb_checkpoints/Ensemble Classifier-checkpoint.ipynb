{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras import optimizers\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import regularizers\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import cv2\n",
    "from keras.optimizers import SGD,RMSprop, adam, Adagrad, Adadelta, Adamax, Nadam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nambu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\nambu\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nambu\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual_model = load_model('../weights/visual_network.h5')\n",
    "audio_model = load_model('../weights/audio_network.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from speech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_paths=[]\n",
    "audio_labels=[]\n",
    "audio_data = '../Data/training/Speech_Data'\n",
    "exp = os.listdir(audio_data)\n",
    "for i in exp:\n",
    "    files = os.listdir(audio_data+'/'+i)\n",
    "    for j in files:\n",
    "        audio_file_paths.append(audio_data+'/'+i+'/'+j)\n",
    "        audio_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "    return mfccsscaled\n",
    "features = []\n",
    "# Iterate through each sound file and extract the features \n",
    "for i in range(0,len(audio_file_paths)) :\n",
    "    file_name = audio_file_paths[i]\n",
    "    class_label = audio_labels[i]\n",
    "    data = extract_features(file_name)\n",
    "    features.append([data, class_label])\n",
    "    \n",
    "audio_df = pd.DataFrame(features, columns=['Audio_Features','Expression_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Angry': 0, 'Disgust': 1, 'Fearful': 2, 'Happy': 3, 'Neutral': 4, 'Sad': 5, 'Surprised': 6}\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "audio_y = le.fit_transform(audio_df.Expression_Label)\n",
    "keys = le.classes_\n",
    "values = le.transform(le.classes_)\n",
    "dictionary = dict(zip(keys, values))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting audio data into train and test\n",
    "audio_X = np.array(audio_df.Audio_Features.tolist()).reshape(-1,40,1)\n",
    "audio_x_train, audio_x_test, audio_y_train, audio_y_test = train_test_split(audio_X, audio_y,  test_size=0.2, stratify=audio_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from visual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_data_path = '../Data/training/Visual_Data'\n",
    "Exp_list = os.listdir(visual_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape -  (4200, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "img_data_list=[]\n",
    "visual_labels = []\n",
    "for dataset in Exp_list:\n",
    "    img_list=os.listdir(visual_data_path+'/'+ dataset)\n",
    "    #print ('Loaded ' + str(len(img_list)) +' images of Expression - '+'{}\\n'.format(dataset))\n",
    "    count =0 \n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(visual_data_path + '/'+ dataset + '/'+ img )\n",
    "        #input_img_gray = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "        input_img_resize=cv2.resize(input_img,(256,256))\n",
    "        img_data_list.append(input_img_resize)\n",
    "        visual_labels.append(dataset)\n",
    "visual_data = np.array(img_data_list)\n",
    "visual_data = visual_data.astype('float32')\n",
    "visual_data = visual_data/255\n",
    "print('Data Shape - ',visual_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'neutral': 4, 'sadness': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "vle = LabelEncoder()\n",
    "visual_y = vle.fit_transform(visual_labels)\n",
    "keys = vle.classes_\n",
    "values = vle.transform(vle.classes_)\n",
    "dictionary = dict(zip(keys, values))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting visual data into train and test\n",
    "visual_X_train, visual_X_test, visual_Y_train, visual_Y_test = train_test_split(visual_data, visual_y, test_size=0.2, stratify=visual_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 480, 4: 480, 3: 480, 5: 480, 2: 480, 1: 480, 0: 480})\n",
      "Counter({0: 480, 6: 480, 5: 480, 4: 480, 2: 480, 3: 480, 1: 480})\n",
      "Counter({4: 120, 6: 120, 3: 120, 5: 120, 2: 120, 1: 120, 0: 120})\n",
      "Counter({0: 120, 1: 120, 4: 120, 5: 120, 2: 120, 6: 120, 3: 120})\n"
     ]
    }
   ],
   "source": [
    "#checking value counts for each expression\n",
    "print(Counter(audio_y_train))\n",
    "print(Counter(visual_Y_train))\n",
    "print(Counter(audio_y_test))\n",
    "print(Counter(visual_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual train data Shape -  (3360, 256, 256, 3)\n",
      "Visual test data Shape -  (840, 256, 256, 3)\n",
      "Audio train data Shape -  (3360, 40, 1)\n",
      "Audio test data Shape -  (840, 40, 1)\n",
      "visual data labels Shape -  (3360, 7)\n",
      "visual data labels Shape -  (840, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Visual train data Shape - ',visual_X_train.shape)\n",
    "print('Visual test data Shape - ',visual_X_test.shape)\n",
    "print('Audio train data Shape - ',audio_x_train.shape)\n",
    "print('Audio test data Shape - ',audio_x_test.shape)\n",
    "\n",
    "visual_Y_traincat= to_categorical(visual_Y_train)\n",
    "visual_Y_testcat= to_categorical(visual_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualpreds_train = visual_model.predict(visual_X_train)\n",
    "visualpreds_test = visual_model.predict(visual_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiopreds_train = audio_model.predict(audio_x_train)\n",
    "audiopreds_test = audio_model.predict(audio_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new data from results of two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audiotraindf = pd.DataFrame(audiopreds_train,columns=['angerAN','disgustAN','fearAN', 'joyAN', 'neutralAN', 'sadnessAN', 'surpriseAN'])\n",
    "Audiotraindf['labelA'] = audio_y_train\n",
    "Audiotraindf = Audiotraindf.sort_values('labelA')\n",
    "Audiotraindf.reset_index(drop=True, inplace=True)\n",
    "Audiotestdf = pd.DataFrame(audiopreds_test,columns=['angerAN','disgustAN','fearAN', 'joyAN', 'neutralAN', 'sadnessAN', 'surpriseAN'])\n",
    "Audiotestdf['labelA'] = audio_y_test\n",
    "Audiotestdf = Audiotestdf.sort_values('labelA')\n",
    "Audiotestdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualtraindf = pd.DataFrame(visualpreds_train,columns=['angerVN','disgustVN','fearVN', 'joyVN', 'neutralVN', 'sadnessVN', 'surpriseVN'])\n",
    "Visualtraindf['labelV'] = visual_Y_train\n",
    "Visualtraindf = Visualtraindf.sort_values('labelV')\n",
    "Visualtraindf.reset_index(drop=True, inplace=True)\n",
    "Visualtestdf = pd.DataFrame(visualpreds_test,columns=['angerVN','disgustVN','fearVN', 'joyVN', 'neutralVN', 'sadnessVN', 'surpriseVN'])\n",
    "Visualtestdf['labelV'] = visual_Y_test\n",
    "Visualtestdf = Visualtestdf.sort_values('labelV')\n",
    "Visualtestdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [Audiotestdf,Visualtestdf]\n",
    "testdata = pd.concat(frames,axis=1)\n",
    "testdata = testdata.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [Audiotraindf,Visualtraindf]\n",
    "traindata = pd.concat(frames,axis=1)\n",
    "traindata =traindata.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = traindata.drop(['labelA','labelV'],axis=1)\n",
    "y_train =traindata['labelA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = testdata.drop(['labelA','labelV'],axis=1)\n",
    "y_test =testdata['labelA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(criterion='entropy',max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9821428571428571\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(x_test,y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = \"../weights/ensemble.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
