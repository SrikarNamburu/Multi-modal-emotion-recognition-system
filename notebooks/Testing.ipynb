{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import moviepy\n",
    "import moviepy.editor\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import import_ipynb\n",
    "from scipy import stats\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testfilepath = '../Data/testing/happy_ramdas.mp4'\n",
    "AudioFilepath = '../Data/testing/audio'\n",
    "Framespath = '../Data/testing/frames'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting .wav and frames from .mp4 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ../Data/testing/audio/audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "def extractDetails(path):\n",
    "    video = moviepy.editor.VideoFileClip(path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(AudioFilepath+'/audio.wav')\n",
    "    vidObj = cv2.VideoCapture(path) \n",
    "    count = 0\n",
    "    # checks whether frames were extracted \n",
    "    success = 1\n",
    "    while success: \n",
    "        try:             \n",
    "            # vidObj object calls read and function extract frames \n",
    "            success, image = vidObj.read() \n",
    "            # Saves the frames with frame-count \n",
    "            cv2.imwrite(Framespath+\"//frame%d.jpg\" % count, image) \n",
    "            count += 1\n",
    "        except:\n",
    "            Done = True\n",
    "extractDetails(Testfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions = {'anger': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sadness': 5, 'surprise': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(Framespath)\n",
    "rdmnum = random.sample(range(0, len(files)), 9)\n",
    "img = []\n",
    "for ind in rdmnum:\n",
    "    img.append(files[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detects the face in image using HAAR cascade then crop it then resize it and finally save it.\n",
    "face_cascade = cv2.CascadeClassifier('../Data/training/haarcascade_frontalface_default.xml') \n",
    "def face_det_crop_resize(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    try:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    except Exception as e:\n",
    "        print(img_path)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        face_clip = img[y:y+h, x:x+w]  #cropping the face in image\n",
    "        cv2.imwrite(img_path, cv2.resize(face_clip, (350, 350)))  #resizing image then saving it\n",
    "for image in img:\n",
    "    face_det_crop_resize(Framespath+'/'+image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loding images and audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data_list=[]\n",
    "for image in img:\n",
    "    input_img=cv2.imread(Framespath + '/'+ image )\n",
    "    input_img_resize=cv2.resize(input_img,(256,256))\n",
    "    img_data_list.append(input_img_resize)\n",
    "visual_data = np.array(img_data_list)\n",
    "visual_data = visual_data.astype('float32')\n",
    "visual_data = visual_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "    return mfccsscaled\n",
    "features = []\n",
    "audio_file = os.listdir(AudioFilepath)[0]\n",
    "file_name = str(AudioFilepath)+'/'+str(audio_file)\n",
    "features.append(extract_features(file_name))\n",
    "audio_data = np.array(features).reshape(-1,40,1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nambu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\nambu\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nambu\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Loaded saved models\n"
     ]
    }
   ],
   "source": [
    "visual_model = load_model('../weights/visual_network.h5')\n",
    "audio_model = load_model('../weights/audio_network.h5')\n",
    "with open('../weights/ensemble.pkl', 'rb') as file:\n",
    "    ensemble_model = pickle.load(file)\n",
    "print('Loaded saved models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the test data case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualpredclass = visual_model.predict_classes(visual_data)\n",
    "visualpredprobs = visual_model.predict(visual_data)\n",
    "MajClassIndx = np.where(visualpredclass == stats.mode(visualpredclass)[0][0])\n",
    "visualpredprob = visualpredprobs[random.choice(MajClassIndx[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiopredprob = audio_model.predict(audio_data, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf = pd.DataFrame([visualpredprob],columns=['angerVN','disgustVN','fearVN', 'joyVN', 'neutralVN', 'sadnessVN', 'surpriseVN'])\n",
    "adf = pd.DataFrame(audiopredprob,columns=['angerAN','disgustAN','fearAN', 'joyAN', 'neutralAN', 'sadnessAN', 'surpriseAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [vdf,adf]\n",
    "finaldf = pd.concat(frames,axis=1)\n",
    "finaldf = finaldf.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedexp = ensemble_model.predict(finaldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Detected by Ensemble Model: disgust\n"
     ]
    }
   ],
   "source": [
    "for exp, label in expressions.items():\n",
    "    if label == predictedexp[0]:\n",
    "        print('Emotion Detected by Ensemble Model:', exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
